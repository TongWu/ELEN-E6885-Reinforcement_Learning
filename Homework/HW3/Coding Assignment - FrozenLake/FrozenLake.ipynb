{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3OixcuRWfNF"
   },
   "source": [
    "## Mount the Google Drive onto the Colab as the storage location.\n",
    "\n",
    "Following the instructions returned from the below cell. You will click a web link and select the google account you want to mount, then copy the authorication code to the blank, press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1hg6sZyWnPL"
   },
   "outputs": [],
   "source": [
    "# This must be run within a Google Colab environment \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OokvpclldFSu"
   },
   "source": [
    "## Append the directory location where you upload the start code folder (In this problem, *RLalgs*) to the sys.path\n",
    "\n",
    "E.g. dir = '/content/drive/My Drive/RL/.', start code folder is inside \"RL\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U53Bo1zEzMq"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/content/gdrive/My Drive/RL/.')\n",
    "sys.path.append('</dir/to/start/code/folder/.>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MK7mp6DNWXw8"
   },
   "source": [
    "Your code should remain in the block marked by<br />\n",
    "\\############################<br />\n",
    "\\# YOUR CODE STARTS HERE<br />\n",
    "\\# YOUR CODE ENDS HERE<br />\n",
    "\\############################<br />\n",
    "Please don't edit anything outside the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-bkZGYkWXw9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9gHp6NAWXxF"
   },
   "source": [
    "## 1. Incremental Implementation of Average\n",
    "We've finished the incremental implementation of average for you. Please call the function estimate with 1/step step size and fixed step size to compare the difference between this two on a simulated Bandit problem.<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjXztwInWXxF"
   },
   "outputs": [],
   "source": [
    "from RLalgs.utils import estimate\n",
    "random.seed(6885)\n",
    "numTimeStep = 10000\n",
    "q_h = np.zeros(numTimeStep + 1) # Q Value estimate with 1/step step size\n",
    "q_f = np.zeros(numTimeStep + 1) # Q value estimate with fixed step size\n",
    "FixedStepSize = 0.5 #A large number to exaggerate the difference\n",
    "for step in range(1, numTimeStep + 1):\n",
    "    if step < numTimeStep / 2:\n",
    "        r = random.gauss(mu = 1, sigma = 0.1)\n",
    "    else:\n",
    "        r = random.gauss(mu = 3, sigma = 0.1)\n",
    "    \n",
    "    #TIPS: Call function estimate defined in ./RLalgs/utils.py\n",
    "    ############################\n",
    "    # YOUR CODE STARTS HERE\n",
    "    q_h[step] = None\n",
    "    q_f[step] = None\n",
    "    # YOUR CODE ENDS HERE\n",
    "    ############################\n",
    "    \n",
    "q_h = q_h[1:]\n",
    "q_f = q_f[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qLlFr3VWXxM"
   },
   "source": [
    "Plot the two Q value estimates. (Please include a title, labels on both axes, and legends)<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhWYCi08WXxO"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-taF6umWXxU"
   },
   "source": [
    "## 2. $\\epsilon$-Greedy for Exploration\n",
    "In Reinforcement Learning, we are always faced with the dilemma of exploration and exploitation. $\\epsilon$-Greedy is a trade-off between them. You are gonna implement Greedy and $\\epsilon$-Greedy. We combine these two policies in one function by treating Greedy as $\\epsilon$-Greedy where $\\epsilon = 0$. Edit the function epsilon_greedy in ./RLalgs/utils.py.<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0N5VJAGWXxV"
   },
   "outputs": [],
   "source": [
    "from RLalgs.utils import epsilon_greedy\n",
    "np.random.seed(6885) #Set the seed to cancel the randomness\n",
    "q = np.random.normal(0, 1, size = 5)\n",
    "############################\n",
    "# YOUR CODE STARTS HERE\n",
    "greedy_action = None #Use epsilon = 0 for Greedy\n",
    "e_greedy_action = None #Use epsilon = 0.1\n",
    "# YOUR CODE ENDS HERE\n",
    "############################\n",
    "print('Values:')\n",
    "print(q)\n",
    "print('Greedy Choice =', greedy_action)\n",
    "print('Epsilon-Greedy Choice =', e_greedy_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMZNXHPWWXxg"
   },
   "source": [
    "You should get the following results.<br />\n",
    "Values:<br />\n",
    "\\[ 0.61264537  0.27923079 -0.84600857  0.05469574 -1.09990968\\]<br />\n",
    "Greedy Choice = 0<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80csLoFuWXxg"
   },
   "source": [
    "## 3. Frozen Lake Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ag1egsO0WXxh"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raHDV2igWXxl"
   },
   "source": [
    "### 3.1 Derive Q value from V value\n",
    "Edit function action_evaluation in ./RLalgs/utils.py.<br />\n",
    "TIPS: $q(s, a)=\\sum_{s',r}p(s',r|s,a)(r+\\gamma v(s'))$<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjiPzPVzWXxs"
   },
   "outputs": [],
   "source": [
    "from RLalgs.utils import action_evaluation\n",
    "v = np.ones(16)\n",
    "q = action_evaluation(env = env.env, gamma = 1, v = v)\n",
    "print('Action values:')\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naD8LRw7WXy_"
   },
   "source": [
    "You should get Q values all equal to one except at State 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VjtGfJkCWXzA"
   },
   "source": [
    "Pseudo-codes of the following four algorithms can be found on Page 80, 83, 130, 131 of the Sutton's book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npMqmLkzWXzB"
   },
   "source": [
    "### 3.2 Model-based RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTkYNUuJWXzC"
   },
   "outputs": [],
   "source": [
    "from RLalgs.utils import action_evaluation, action_selection, render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8BWq8PLWXzH"
   },
   "source": [
    "### 3.2.1 Policy Iteration\n",
    "Edit the function policy_iteration and relevant functions in ./RLalgs/pi.py to implement the Policy Iteration Algorithm.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYEuSethWXzI"
   },
   "outputs": [],
   "source": [
    "from RLalgs.pi import policy_iteration\n",
    "V, policy, numIterations = policy_iteration(env = env.env, gamma = 1, max_iteration = 500, theta = 1e-7)\n",
    "print('State values:')\n",
    "print(V)\n",
    "print('Number of iterations to converge =', numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4q6PmhZZWXzS"
   },
   "source": [
    "You should get values close to:<br />\n",
    "State values:<br />\n",
    "\\[0.82352774 0.8235272  0.82352682 0.82352662 0.82352791 0.<br />\n",
    "0.52941063 0.         0.82352817 0.82352851 0.76470509 0.<br />0.         0.88235232 0.94117615 0.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olPkpHkhWXzT"
   },
   "outputs": [],
   "source": [
    "#Uncomment and run the following to evaluate your result, comment them when you generate the pdf\n",
    "#Q = action_evaluation(env = env.env, gamma = 1, v = V)\n",
    "#policy_estimate = action_selection(Q)\n",
    "#render(env, policy_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gutmkVWaWXzi"
   },
   "source": [
    "### 3.2.2 Value Iteration\n",
    "Edit the function value_iteration and relevant functions in ./RLalgs/vi.py to implement the Value Iteration Algorithm.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0j9ZEkPyWXzj"
   },
   "outputs": [],
   "source": [
    "from RLalgs.vi import value_iteration\n",
    "V, policy, numIterations = value_iteration(env = env.env, gamma = 1, max_iteration = 500, theta = 1e-7)\n",
    "print('State values:')\n",
    "print(V)\n",
    "print('Number of iterations to converge =', numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUTMsRrxWXzr"
   },
   "source": [
    "You should get values close to:<br />\n",
    "State values:<br />\n",
    "\\[0.82352773 0.82352718  0.8235268 0.8235266 0.8235279 0.<br />\n",
    "0.52941062 0.         0.82352816 0.8235285 0.76470509 0.<br />0.         0.88235231 0.94117615 0.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNcCXWuzWXzs"
   },
   "outputs": [],
   "source": [
    "#Uncomment and run the following to evaluate your result, comment them when you generate the pdf\n",
    "#Q = action_evaluation(env = env.env, gamma = 1, v = V)\n",
    "#policy_estimate = action_selection(Q)\n",
    "#render(env, policy_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_InsqQe3WXzw"
   },
   "source": [
    "### 3.3 Model free RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNir2YPgWXzw"
   },
   "source": [
    "### 3.3.1 Q-Learning\n",
    "Edit the function QLearning in ./RLalgs/ql.py to implement the Q-Learning Algorithm.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4jpOau5WXzx"
   },
   "outputs": [],
   "source": [
    "from RLalgs.ql import QLearning\n",
    "Q = QLearning(env = env.env, num_episodes = 1000, gamma = 1, lr = 0.1, e = 0.1)\n",
    "print('Action values:')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhRDkVUjWX0D"
   },
   "source": [
    "Generally, you should get non-zero action values on non-terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JW8ZbutmWX0E"
   },
   "outputs": [],
   "source": [
    "#Uncomment the following to evaluate your result, comment them when you generate the pdf\n",
    "#env = gym.make('FrozenLake-v1')\n",
    "#policy_estimate = action_selection(Q)\n",
    "#render(env, policy_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw5I3ZXdWX0G"
   },
   "source": [
    "### 3.3.2 SARSA\n",
    "Edit the function SARSA in ./RLalgs/sarsa.py to implement the SARSA Algorithm.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-bvEnPuWX0H"
   },
   "outputs": [],
   "source": [
    "from RLalgs.sarsa import SARSA\n",
    "Q = SARSA(env = env.env, num_episodes = 1000, gamma = 1, lr = 0.1, e = 0.1)\n",
    "print('Action values:')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhaj57nZWX0L"
   },
   "source": [
    "Generally, you should get non-zero action values on non-terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2X3qsekoWX0M"
   },
   "outputs": [],
   "source": [
    "#Uncomment the following to evaluate your result, comment them when you generate the pdf\n",
    "#env = gym.make('FrozenLake-v1')\n",
    "#policy_estimate = action_selection(Q)\n",
    "#render(env, policy_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVArXevGWX0P"
   },
   "source": [
    "### 3.4 Human\n",
    "You can play this game if you are interested. See if you can get the frisbee either with or without the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2zKGeoSWX0Q"
   },
   "outputs": [],
   "source": [
    "from RLalgs.utils import human_play\n",
    "#Uncomment and run the following to play the game, comment it when you generate the pdf\n",
    "#env = gym.make('FrozenLake-v1')\n",
    "#human_play(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uHP9HIrWX0S"
   },
   "source": [
    "## 4. Exploration VS. Exploitation\n",
    "Try to reproduce Figure 2.2 (the upper one is enough) of the Sutton's book based on the experiment described in [Chapter 2.3](http://incompleteideas.net/book/RLbook2020.pdf).<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RJFKpAVWX0T"
   },
   "outputs": [],
   "source": [
    "# Do the experiment and record average reward acquired in each time step\n",
    "############################\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxQ5tSs0WX0V"
   },
   "outputs": [],
   "source": [
    "# Plot the average reward\n",
    "############################\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMfbvo9oWX0Y"
   },
   "source": [
    "You should get curves similar to that in the book."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FrozenLake.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
